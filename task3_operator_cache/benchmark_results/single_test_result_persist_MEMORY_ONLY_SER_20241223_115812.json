{
  "timestamp": "20241223_115812",
  "duration": 35.62267589569092,
  "success": false,
  "application_id": null,
  "stdout": "Error occurred: type object 'StorageLevel' has no attribute 'MEMORY_ONLY_SER'\nTraceback (most recent call last):\n  File \"/usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY_SER.py\", line 37, in main\n    customers.persist(StorageLevel.MEMORY_ONLY_SER)  # \u4f7f\u7528 persist() \u5e76\u6307\u5b9a\u5b58\u50a8\u7ea7\u522b\nAttributeError: type object 'StorageLevel' has no attribute 'MEMORY_ONLY_SER'. Did you mean: 'MEMORY_ONLY_2'?\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY_SER.py\", line 126, in <module>\n    main()\n  File \"/usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY_SER.py\", line 121, in main\n    orders.unpersist()\nUnboundLocalError: local variable 'orders' referenced before assignment\n",
  "stderr": "24/12/23 11:58:15 INFO SparkContext: Running Spark version 3.4.4\n24/12/23 11:58:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n24/12/23 11:58:16 INFO ResourceUtils: ==============================================================\n24/12/23 11:58:16 INFO ResourceUtils: No custom resources configured for spark.driver.\n24/12/23 11:58:16 INFO ResourceUtils: ==============================================================\n24/12/23 11:58:16 INFO SparkContext: Submitted application: TPCH_Query3_persist_MEMORY_ONLY_SER\n24/12/23 11:58:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 512, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n24/12/23 11:58:16 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n24/12/23 11:58:16 INFO ResourceProfileManager: Added ResourceProfile id: 0\n24/12/23 11:58:16 INFO SecurityManager: Changing view acls to: hdfs\n24/12/23 11:58:16 INFO SecurityManager: Changing modify acls to: hdfs\n24/12/23 11:58:16 INFO SecurityManager: Changing view acls groups to: \n24/12/23 11:58:16 INFO SecurityManager: Changing modify acls groups to: \n24/12/23 11:58:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hdfs; groups with view permissions: EMPTY; users with modify permissions: hdfs; groups with modify permissions: EMPTY\n24/12/23 11:58:16 INFO Utils: Successfully started service 'sparkDriver' on port 34531.\n24/12/23 11:58:16 INFO SparkEnv: Registering MapOutputTracker\n24/12/23 11:58:17 INFO SparkEnv: Registering BlockManagerMaster\n24/12/23 11:58:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n24/12/23 11:58:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n24/12/23 11:58:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n24/12/23 11:58:17 INFO DiskBlockManager: Created local directory at /usr/local/spark3.4.4/blockmgr-91cc559c-9b92-4665-a9aa-49bf1fd43151\n24/12/23 11:58:17 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n24/12/23 11:58:17 INFO SparkEnv: Registering OutputCommitCoordinator\n24/12/23 11:58:17 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n24/12/23 11:58:17 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n24/12/23 11:58:18 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at master/172.17.0.2:8032\n24/12/23 11:58:19 INFO Configuration: resource-types.xml not found\n24/12/23 11:58:19 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n24/12/23 11:58:19 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)\n24/12/23 11:58:19 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n24/12/23 11:58:19 INFO Client: Setting up container launch context for our AM\n24/12/23 11:58:19 INFO Client: Setting up the launch environment for our AM container\n24/12/23 11:58:19 INFO Client: Preparing resources for our AM container\n24/12/23 11:58:19 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n24/12/23 11:58:22 INFO Client: Uploading resource file:/usr/local/spark3.4.4/spark-9fdc6799-4fa6-4389-99e5-8340223fa5a9/__spark_libs__4233929590985246319.zip -> hdfs://master:9000/user/hdfs/.sparkStaging/application_1734876530876_0015/__spark_libs__4233929590985246319.zip\n24/12/23 11:58:25 INFO Client: Uploading resource file:/usr/local/spark3.4.4/python/lib/pyspark.zip -> hdfs://master:9000/user/hdfs/.sparkStaging/application_1734876530876_0015/pyspark.zip\n24/12/23 11:58:25 INFO Client: Uploading resource file:/usr/local/spark3.4.4/python/lib/py4j-0.10.9.7-src.zip -> hdfs://master:9000/user/hdfs/.sparkStaging/application_1734876530876_0015/py4j-0.10.9.7-src.zip\n24/12/23 11:58:25 INFO Client: Uploading resource file:/usr/local/spark3.4.4/spark-9fdc6799-4fa6-4389-99e5-8340223fa5a9/__spark_conf__9069028375299132258.zip -> hdfs://master:9000/user/hdfs/.sparkStaging/application_1734876530876_0015/__spark_conf__.zip\n24/12/23 11:58:26 INFO SecurityManager: Changing view acls to: hdfs\n24/12/23 11:58:26 INFO SecurityManager: Changing modify acls to: hdfs\n24/12/23 11:58:26 INFO SecurityManager: Changing view acls groups to: \n24/12/23 11:58:26 INFO SecurityManager: Changing modify acls groups to: \n24/12/23 11:58:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hdfs; groups with view permissions: EMPTY; users with modify permissions: hdfs; groups with modify permissions: EMPTY\n24/12/23 11:58:26 INFO Client: Submitting application application_1734876530876_0015 to ResourceManager\n24/12/23 11:58:26 INFO YarnClientImpl: Submitted application application_1734876530876_0015\n24/12/23 11:58:27 INFO Client: Application report for application_1734876530876_0015 (state: ACCEPTED)\n24/12/23 11:58:27 INFO Client: \n\t client token: N/A\n\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n\t ApplicationMaster host: N/A\n\t ApplicationMaster RPC port: -1\n\t queue: root.default\n\t start time: 1734955106066\n\t final status: UNDEFINED\n\t tracking URL: http://master:8088/proxy/application_1734876530876_0015/\n\t user: hdfs\n24/12/23 11:58:28 INFO Client: Application report for application_1734876530876_0015 (state: ACCEPTED)\n24/12/23 11:58:29 INFO Client: Application report for application_1734876530876_0015 (state: ACCEPTED)\n24/12/23 11:58:30 INFO Client: Application report for application_1734876530876_0015 (state: ACCEPTED)\n24/12/23 11:58:31 INFO Client: Application report for application_1734876530876_0015 (state: ACCEPTED)\n24/12/23 11:58:32 INFO Client: Application report for application_1734876530876_0015 (state: ACCEPTED)\n24/12/23 11:58:33 INFO Client: Application report for application_1734876530876_0015 (state: ACCEPTED)\n24/12/23 11:58:34 INFO Client: Application report for application_1734876530876_0015 (state: RUNNING)\n24/12/23 11:58:34 INFO Client: \n\t client token: N/A\n\t diagnostics: N/A\n\t ApplicationMaster host: 172.17.0.4\n\t ApplicationMaster RPC port: -1\n\t queue: root.default\n\t start time: 1734955106066\n\t final status: UNDEFINED\n\t tracking URL: http://master:8088/proxy/application_1734876530876_0015/\n\t user: hdfs\n24/12/23 11:58:34 INFO YarnClientSchedulerBackend: Application application_1734876530876_0015 has started running.\n24/12/23 11:58:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43473.\n24/12/23 11:58:34 INFO NettyBlockTransferService: Server created on master:43473\n24/12/23 11:58:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n24/12/23 11:58:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, master, 43473, None)\n24/12/23 11:58:34 INFO BlockManagerMasterEndpoint: Registering block manager master:43473 with 366.3 MiB RAM, BlockManagerId(driver, master, 43473, None)\n24/12/23 11:58:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, master, 43473, None)\n24/12/23 11:58:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, master, 43473, None)\n24/12/23 11:58:34 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> master, PROXY_URI_BASES -> http://master:8088/proxy/application_1734876530876_0015), /proxy/application_1734876530876_0015\n24/12/23 11:58:34 INFO SingleEventLogFileWriter: Logging events to hdfs://master:9000/spark-history/application_1734876530876_0015.inprogress\n24/12/23 11:58:34 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:34 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:34 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:34 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:34 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:34 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:34 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:34 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:34 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:34 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:34 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:34 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:34 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:34 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:34 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:34 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:34 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:35 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:35 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:35 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:35 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:35 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n24/12/23 11:58:35 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:35 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:35 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:35 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:35 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:58:43 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.0.4:49626) with ID 1,  ResourceProfileId 0\n24/12/23 11:58:43 INFO BlockManagerMasterEndpoint: Registering block manager slave02:42135 with 93.3 MiB RAM, BlockManagerId(1, slave02, 42135, None)\n24/12/23 11:58:46 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.0.3:56234) with ID 2,  ResourceProfileId 0\n24/12/23 11:58:46 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n24/12/23 11:58:46 INFO BlockManagerMasterEndpoint: Registering block manager slave01:42587 with 93.3 MiB RAM, BlockManagerId(2, slave01, 42587, None)\n24/12/23 11:58:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 364.8 KiB, free 365.9 MiB)\n24/12/23 11:58:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.0 KiB, free 365.9 MiB)\n24/12/23 11:58:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on master:43473 (size: 33.0 KiB, free: 366.3 MiB)\n24/12/23 11:58:46 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n24/12/23 11:58:47 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 364.8 KiB, free 365.6 MiB)\n24/12/23 11:58:47 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.0 KiB, free 365.5 MiB)\n24/12/23 11:58:47 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on master:43473 (size: 33.0 KiB, free: 366.2 MiB)\n24/12/23 11:58:47 INFO SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:0\n24/12/23 11:58:47 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 364.8 KiB, free 365.2 MiB)\n24/12/23 11:58:47 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.0 KiB, free 365.1 MiB)\n24/12/23 11:58:47 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on master:43473 (size: 33.0 KiB, free: 366.2 MiB)\n24/12/23 11:58:47 INFO SparkContext: Created broadcast 2 from textFile at NativeMethodAccessorImpl.java:0\n24/12/23 11:58:47 INFO PythonRDD: Removing RDD 6 from persistence list\n24/12/23 11:58:47 INFO BlockManager: Removing RDD 6\n24/12/23 11:58:47 INFO SparkContext: Invoking stop() from shutdown hook\n24/12/23 11:58:47 INFO SparkContext: SparkContext is stopping with exitCode 0.\n24/12/23 11:58:47 INFO SparkUI: Stopped Spark web UI at http://master:4040\n24/12/23 11:58:47 INFO YarnClientSchedulerBackend: Interrupting monitor thread\n24/12/23 11:58:47 INFO YarnClientSchedulerBackend: Shutting down all executors\n24/12/23 11:58:47 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\n24/12/23 11:58:47 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped\n24/12/23 11:58:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n24/12/23 11:58:47 INFO MemoryStore: MemoryStore cleared\n24/12/23 11:58:47 INFO BlockManager: BlockManager stopped\n24/12/23 11:58:47 INFO BlockManagerMaster: BlockManagerMaster stopped\n24/12/23 11:58:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n24/12/23 11:58:47 INFO SparkContext: Successfully stopped SparkContext\n24/12/23 11:58:47 INFO ShutdownHookManager: Shutdown hook called\n24/12/23 11:58:47 INFO ShutdownHookManager: Deleting directory /usr/local/spark3.4.4/spark-9fdc6799-4fa6-4389-99e5-8340223fa5a9/pyspark-29ca7530-52ff-4e20-84ea-73b284a4b136\n24/12/23 11:58:47 INFO ShutdownHookManager: Deleting directory /usr/local/spark3.4.4/spark-9fdc6799-4fa6-4389-99e5-8340223fa5a9\n24/12/23 11:58:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-485206af-bbfc-4dce-b5f6-3bd3c5c95dee\n"
}