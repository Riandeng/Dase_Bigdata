{
  "timestamp": "20241223_025812",
  "duration": 65.03695392608643,
  "success": false,
  "application_id": "application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/worker.py\",",
  "stdout": "Error occurred: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 11) (slave01 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py\", line 52, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2790)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2726)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2725)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2725)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1211)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1211)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1211)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2989)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2928)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2917)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:976)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2258)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2298)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1022)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:408)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1021)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py\", line 52, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n\nTraceback (most recent call last):\n  File \"/usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py\", line 121, in <module>\n    main()\n  File \"/usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py\", line 69, in main\n    .sortBy(lambda x: (-x[1], x[2]))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1559, in sortBy\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1495, in sortByKey\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2297, in count\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2272, in sum\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2025, in fold\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1814, in collect\n  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 11) (slave01 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py\", line 52, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2790)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2726)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2725)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2725)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1211)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1211)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1211)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2989)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2928)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2917)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:976)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2258)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2298)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1022)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:408)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1021)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py\", line 52, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n\n",
  "stderr": "24/12/23 02:58:16 INFO SparkContext: Running Spark version 3.4.4\n24/12/23 02:58:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n24/12/23 02:58:16 INFO ResourceUtils: ==============================================================\n24/12/23 02:58:16 INFO ResourceUtils: No custom resources configured for spark.driver.\n24/12/23 02:58:16 INFO ResourceUtils: ==============================================================\n24/12/23 02:58:16 INFO SparkContext: Submitted application: TPCH_Query3_Broadcast\n24/12/23 02:58:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n24/12/23 02:58:16 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n24/12/23 02:58:16 INFO ResourceProfileManager: Added ResourceProfile id: 0\n24/12/23 02:58:16 INFO SecurityManager: Changing view acls to: hdfs\n24/12/23 02:58:16 INFO SecurityManager: Changing modify acls to: hdfs\n24/12/23 02:58:16 INFO SecurityManager: Changing view acls groups to: \n24/12/23 02:58:16 INFO SecurityManager: Changing modify acls groups to: \n24/12/23 02:58:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hdfs; groups with view permissions: EMPTY; users with modify permissions: hdfs; groups with modify permissions: EMPTY\n24/12/23 02:58:16 INFO Utils: Successfully started service 'sparkDriver' on port 36103.\n24/12/23 02:58:17 INFO SparkEnv: Registering MapOutputTracker\n24/12/23 02:58:17 INFO SparkEnv: Registering BlockManagerMaster\n24/12/23 02:58:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n24/12/23 02:58:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n24/12/23 02:58:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n24/12/23 02:58:17 INFO DiskBlockManager: Created local directory at /usr/local/spark3.4.4/blockmgr-8ca59e3a-3cbf-4841-97f2-1026249e83d0\n24/12/23 02:58:17 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n24/12/23 02:58:17 INFO SparkEnv: Registering OutputCommitCoordinator\n24/12/23 02:58:17 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n24/12/23 02:58:17 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n24/12/23 02:58:18 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at master/172.17.0.2:8032\n24/12/23 02:58:19 INFO Configuration: resource-types.xml not found\n24/12/23 02:58:19 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n24/12/23 02:58:19 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)\n24/12/23 02:58:19 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n24/12/23 02:58:19 INFO Client: Setting up container launch context for our AM\n24/12/23 02:58:19 INFO Client: Setting up the launch environment for our AM container\n24/12/23 02:58:19 INFO Client: Preparing resources for our AM container\n24/12/23 02:58:19 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n24/12/23 02:58:21 INFO Client: Uploading resource file:/usr/local/spark3.4.4/spark-63ecbba0-d7ea-4dee-bcb9-93c294e71d16/__spark_libs__8139800860080705694.zip -> hdfs://master:9000/user/hdfs/.sparkStaging/application_1734876530876_0006/__spark_libs__8139800860080705694.zip\n24/12/23 02:58:23 INFO Client: Uploading resource file:/usr/local/spark3.4.4/python/lib/pyspark.zip -> hdfs://master:9000/user/hdfs/.sparkStaging/application_1734876530876_0006/pyspark.zip\n24/12/23 02:58:23 INFO Client: Uploading resource file:/usr/local/spark3.4.4/python/lib/py4j-0.10.9.7-src.zip -> hdfs://master:9000/user/hdfs/.sparkStaging/application_1734876530876_0006/py4j-0.10.9.7-src.zip\n24/12/23 02:58:24 INFO Client: Uploading resource file:/usr/local/spark3.4.4/spark-63ecbba0-d7ea-4dee-bcb9-93c294e71d16/__spark_conf__2976254432524021813.zip -> hdfs://master:9000/user/hdfs/.sparkStaging/application_1734876530876_0006/__spark_conf__.zip\n24/12/23 02:58:24 INFO SecurityManager: Changing view acls to: hdfs\n24/12/23 02:58:24 INFO SecurityManager: Changing modify acls to: hdfs\n24/12/23 02:58:24 INFO SecurityManager: Changing view acls groups to: \n24/12/23 02:58:24 INFO SecurityManager: Changing modify acls groups to: \n24/12/23 02:58:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hdfs; groups with view permissions: EMPTY; users with modify permissions: hdfs; groups with modify permissions: EMPTY\n24/12/23 02:58:24 INFO Client: Submitting application application_1734876530876_0006 to ResourceManager\n24/12/23 02:58:24 INFO YarnClientImpl: Submitted application application_1734876530876_0006\n24/12/23 02:58:25 INFO Client: Application report for application_1734876530876_0006 (state: ACCEPTED)\n24/12/23 02:58:25 INFO Client: \n\t client token: N/A\n\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n\t ApplicationMaster host: N/A\n\t ApplicationMaster RPC port: -1\n\t queue: root.default\n\t start time: 1734922704877\n\t final status: UNDEFINED\n\t tracking URL: http://master:8088/proxy/application_1734876530876_0006/\n\t user: hdfs\n24/12/23 02:58:26 INFO Client: Application report for application_1734876530876_0006 (state: ACCEPTED)\n24/12/23 02:58:27 INFO Client: Application report for application_1734876530876_0006 (state: ACCEPTED)\n24/12/23 02:58:28 INFO Client: Application report for application_1734876530876_0006 (state: ACCEPTED)\n24/12/23 02:58:29 INFO Client: Application report for application_1734876530876_0006 (state: ACCEPTED)\n24/12/23 02:58:30 INFO Client: Application report for application_1734876530876_0006 (state: ACCEPTED)\n24/12/23 02:58:31 INFO Client: Application report for application_1734876530876_0006 (state: ACCEPTED)\n24/12/23 02:58:32 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> master, PROXY_URI_BASES -> http://master:8088/proxy/application_1734876530876_0006), /proxy/application_1734876530876_0006\n24/12/23 02:58:32 INFO Client: Application report for application_1734876530876_0006 (state: RUNNING)\n24/12/23 02:58:32 INFO Client: \n\t client token: N/A\n\t diagnostics: N/A\n\t ApplicationMaster host: 172.17.0.4\n\t ApplicationMaster RPC port: -1\n\t queue: root.default\n\t start time: 1734922704877\n\t final status: UNDEFINED\n\t tracking URL: http://master:8088/proxy/application_1734876530876_0006/\n\t user: hdfs\n24/12/23 02:58:32 INFO YarnClientSchedulerBackend: Application application_1734876530876_0006 has started running.\n24/12/23 02:58:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45721.\n24/12/23 02:58:33 INFO NettyBlockTransferService: Server created on master:45721\n24/12/23 02:58:33 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n24/12/23 02:58:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, master, 45721, None)\n24/12/23 02:58:33 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n24/12/23 02:58:33 INFO BlockManagerMasterEndpoint: Registering block manager master:45721 with 366.3 MiB RAM, BlockManagerId(driver, master, 45721, None)\n24/12/23 02:58:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, master, 45721, None)\n24/12/23 02:58:33 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, master, 45721, None)\n24/12/23 02:58:33 INFO SingleEventLogFileWriter: Logging events to hdfs://master:9000/spark-history/application_1734876530876_0006.inprogress\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:33 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 02:58:39 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.0.4:58580) with ID 1,  ResourceProfileId 0\n24/12/23 02:58:39 INFO BlockManagerMasterEndpoint: Registering block manager slave02:43567 with 912.3 MiB RAM, BlockManagerId(1, slave02, 43567, None)\n24/12/23 02:58:42 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.0.3:45664) with ID 2,  ResourceProfileId 0\n24/12/23 02:58:42 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n24/12/23 02:58:42 INFO BlockManagerMasterEndpoint: Registering block manager slave01:46215 with 912.3 MiB RAM, BlockManagerId(2, slave01, 46215, None)\n24/12/23 02:58:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 364.8 KiB, free 365.9 MiB)\n24/12/23 02:58:43 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.0 KiB, free 365.9 MiB)\n24/12/23 02:58:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on master:45721 (size: 33.0 KiB, free: 366.3 MiB)\n24/12/23 02:58:43 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n24/12/23 02:58:43 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 364.8 KiB, free 365.6 MiB)\n24/12/23 02:58:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.0 KiB, free 365.5 MiB)\n24/12/23 02:58:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on master:45721 (size: 33.0 KiB, free: 366.2 MiB)\n24/12/23 02:58:43 INFO SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:0\n24/12/23 02:58:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 364.8 KiB, free 365.2 MiB)\n24/12/23 02:58:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.0 KiB, free 365.1 MiB)\n24/12/23 02:58:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on master:45721 (size: 33.0 KiB, free: 366.2 MiB)\n24/12/23 02:58:43 INFO SparkContext: Created broadcast 2 from textFile at NativeMethodAccessorImpl.java:0\n24/12/23 02:58:43 INFO FileInputFormat: Total input files to process : 1\n24/12/23 02:58:43 INFO SparkContext: Starting job: collectAsMap at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py:39\n24/12/23 02:58:43 INFO DAGScheduler: Got job 0 (collectAsMap at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py:39) with 2 output partitions\n24/12/23 02:58:43 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsMap at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py:39)\n24/12/23 02:58:43 INFO DAGScheduler: Parents of final stage: List()\n24/12/23 02:58:43 INFO DAGScheduler: Missing parents: List()\n24/12/23 02:58:43 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[6] at collectAsMap at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py:39), which has no missing parents\n24/12/23 02:58:44 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.7 KiB, free 365.1 MiB)\n24/12/23 02:58:44 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 365.1 MiB)\n24/12/23 02:58:44 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on master:45721 (size: 5.2 KiB, free: 366.2 MiB)\n24/12/23 02:58:44 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1540\n24/12/23 02:58:44 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[6] at collectAsMap at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py:39) (first 15 tasks are for partitions Vector(0, 1))\n24/12/23 02:58:44 INFO YarnScheduler: Adding task set 0.0 with 2 tasks resource profile 0\n24/12/23 02:58:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (slave02, executor 1, partition 0, NODE_LOCAL, 8810 bytes) \n24/12/23 02:58:44 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (slave01, executor 2, partition 1, NODE_LOCAL, 8810 bytes) \n24/12/23 02:58:44 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on slave02:43567 (size: 5.2 KiB, free: 912.3 MiB)\n24/12/23 02:58:44 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on slave01:46215 (size: 5.2 KiB, free: 912.3 MiB)\n24/12/23 02:58:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on slave01:46215 (size: 33.0 KiB, free: 912.3 MiB)\n24/12/23 02:58:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on slave02:43567 (size: 33.0 KiB, free: 912.3 MiB)\n24/12/23 02:58:49 INFO BlockManagerInfo: Added taskresult_0 in memory on slave02:43567 (size: 2.7 MiB, free: 909.6 MiB)\n24/12/23 02:58:49 INFO TransportClientFactory: Successfully created connection to slave02/172.17.0.4:43567 after 24 ms (0 ms spent in bootstraps)\n24/12/23 02:58:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5426 ms on slave02 (executor 1) (1/2)\n24/12/23 02:58:49 INFO BlockManagerInfo: Added taskresult_1 in memory on slave01:46215 (size: 2.7 MiB, free: 909.5 MiB)\n24/12/23 02:58:49 INFO BlockManagerInfo: Removed taskresult_0 on slave02:43567 in memory (size: 2.7 MiB, free: 912.3 MiB)\n24/12/23 02:58:49 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 55841\n24/12/23 02:58:49 INFO TransportClientFactory: Successfully created connection to slave01/172.17.0.3:46215 after 13 ms (0 ms spent in bootstraps)\n24/12/23 02:58:49 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 5513 ms on slave01 (executor 2) (2/2)\n24/12/23 02:58:49 INFO DAGScheduler: ResultStage 0 (collectAsMap at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py:39) finished in 5.680 s\n24/12/23 02:58:49 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n24/12/23 02:58:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n24/12/23 02:58:49 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished\n24/12/23 02:58:49 INFO DAGScheduler: Job 0 finished: collectAsMap at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py:39, took 5.777165 s\n24/12/23 02:58:49 INFO BlockManagerInfo: Removed taskresult_1 on slave01:46215 in memory (size: 2.7 MiB, free: 912.3 MiB)\n24/12/23 02:58:49 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 352.0 B, free 365.1 MiB)\n24/12/23 02:58:49 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.9 MiB, free 362.2 MiB)\n24/12/23 02:58:49 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on master:45721 (size: 2.9 MiB, free: 363.3 MiB)\n24/12/23 02:58:49 INFO SparkContext: Created broadcast 4 from broadcast at NativeMethodAccessorImpl.java:0\n24/12/23 02:58:49 INFO FileInputFormat: Total input files to process : 1\n24/12/23 02:58:49 INFO NetworkTopology: Adding a new node: /default-rack/172.17.0.4:9866\n24/12/23 02:58:49 INFO NetworkTopology: Adding a new node: /default-rack/172.17.0.3:9866\n24/12/23 02:58:49 INFO FileInputFormat: Total input files to process : 1\n24/12/23 02:58:50 INFO SparkContext: Starting job: sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py:69\n24/12/23 02:58:50 INFO DAGScheduler: Registering RDD 11 (join at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py:56) as input to shuffle 1\n24/12/23 02:58:50 INFO DAGScheduler: Registering RDD 15 (reduceByKey at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py:65) as input to shuffle 0\n24/12/23 02:58:50 INFO DAGScheduler: Got job 1 (sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py:69) with 8 output partitions\n24/12/23 02:58:50 INFO DAGScheduler: Final stage: ResultStage 3 (sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py:69)\n24/12/23 02:58:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n24/12/23 02:58:50 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)\n24/12/23 02:58:50 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[11] at join at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py:56), which has no missing parents\n24/12/23 02:58:50 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 20.2 KiB, free 362.2 MiB)\n24/12/23 02:58:50 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 362.2 MiB)\n24/12/23 02:58:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on master:45721 (size: 9.2 KiB, free: 363.3 MiB)\n24/12/23 02:58:50 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1540\n24/12/23 02:58:50 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 1 (PairwiseRDD[11] at join at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py:56) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n24/12/23 02:58:50 INFO YarnScheduler: Adding task set 1.0 with 8 tasks resource profile 0\n24/12/23 02:58:50 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (slave01, executor 2, partition 0, NODE_LOCAL, 8906 bytes) \n24/12/23 02:58:50 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (slave02, executor 1, partition 2, NODE_LOCAL, 8908 bytes) \n24/12/23 02:58:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on slave02:43567 (size: 9.2 KiB, free: 912.3 MiB)\n24/12/23 02:58:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on slave01:46215 (size: 9.2 KiB, free: 912.3 MiB)\n24/12/23 02:58:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on slave02:43567 (size: 33.0 KiB, free: 912.2 MiB)\n24/12/23 02:58:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on slave01:46215 (size: 33.0 KiB, free: 912.2 MiB)\n24/12/23 02:58:50 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on slave01:46215 (size: 2.9 MiB, free: 909.3 MiB)\n24/12/23 02:58:50 INFO BlockManagerInfo: Added broadcast_4_python on disk on slave01:46215 (size: 5.3 MiB)\n24/12/23 02:58:51 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (slave01, executor 2, partition 3, NODE_LOCAL, 8908 bytes) \n24/12/23 02:58:51 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 2) (slave01 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py\", line 52, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n\n24/12/23 02:58:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on slave01:46215 (size: 33.0 KiB, free: 909.3 MiB)\n24/12/23 02:59:00 INFO TaskSetManager: Starting task 0.1 in stage 1.0 (TID 5) (slave02, executor 1, partition 0, NODE_LOCAL, 8906 bytes) \n24/12/23 02:59:00 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 10428 ms on slave02 (executor 1) (1/8)\n24/12/23 02:59:00 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on slave02:43567 (size: 33.0 KiB, free: 912.2 MiB)\n24/12/23 02:59:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on slave02:43567 (size: 2.9 MiB, free: 909.3 MiB)\n24/12/23 02:59:00 INFO BlockManagerInfo: Added broadcast_4_python on disk on slave02:43567 (size: 5.3 MiB)\n24/12/23 02:59:01 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 6) (slave01, executor 2, partition 4, NODE_LOCAL, 8908 bytes) \n24/12/23 02:59:01 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 9654 ms on slave01 (executor 2) (2/8)\n24/12/23 02:59:01 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 7) (slave02, executor 1, partition 5, NODE_LOCAL, 8908 bytes) \n24/12/23 02:59:01 WARN TaskSetManager: Lost task 0.1 in stage 1.0 (TID 5) (slave02 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000002/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000002/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000002/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py\", line 52, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n\n24/12/23 02:59:08 INFO TaskSetManager: Starting task 0.2 in stage 1.0 (TID 8) (slave01, executor 2, partition 0, NODE_LOCAL, 8906 bytes) \n24/12/23 02:59:08 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 6) in 7467 ms on slave01 (executor 2) (3/8)\n24/12/23 02:59:08 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 9) (slave02, executor 1, partition 6, NODE_LOCAL, 8908 bytes) \n24/12/23 02:59:08 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 7) in 7708 ms on slave02 (executor 1) (4/8)\n24/12/23 02:59:08 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 10) (slave01, executor 2, partition 7, NODE_LOCAL, 8908 bytes) \n24/12/23 02:59:08 WARN TaskSetManager: Lost task 0.2 in stage 1.0 (TID 8) (slave01 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py\", line 52, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n\n24/12/23 02:59:15 INFO TaskSetManager: Starting task 0.3 in stage 1.0 (TID 11) (slave01, executor 2, partition 0, NODE_LOCAL, 8906 bytes) \n24/12/23 02:59:15 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 10) in 6264 ms on slave01 (executor 2) (5/8)\n24/12/23 02:59:15 INFO TaskSetManager: Lost task 0.3 in stage 1.0 (TID 11) on slave01, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py\", line 52, in <lambda>\nIndexError: tuple index out of range\n) [duplicate 1]\n24/12/23 02:59:15 ERROR TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job\n24/12/23 02:59:15 INFO YarnScheduler: Cancelling stage 1\n24/12/23 02:59:15 INFO YarnScheduler: Killing all running tasks in stage 1: Stage cancelled\n24/12/23 02:59:15 INFO YarnScheduler: Stage 1 was cancelled\n24/12/23 02:59:15 INFO DAGScheduler: ShuffleMapStage 1 (join at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py:56) failed in 25.728 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 11) (slave01 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/hadoop/tmp/nm-local-dir/usercache/hdfs/appcache/application_1734876530876_0006/container_1734876530876_0006_01_000003/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py\", line 52, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n\nDriver stacktrace:\n24/12/23 02:59:15 INFO DAGScheduler: Job 1 failed: sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_broadcast.py:69, took 25.816461 s\n24/12/23 02:59:15 INFO SparkContext: SparkContext is stopping with exitCode 0.\n24/12/23 02:59:15 WARN TaskSetManager: Lost task 6.0 in stage 1.0 (TID 9) (slave02 executor 1): TaskKilled (Stage cancelled)\n24/12/23 02:59:15 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \n24/12/23 02:59:15 INFO SparkUI: Stopped Spark web UI at http://master:4040\n24/12/23 02:59:16 INFO YarnClientSchedulerBackend: Interrupting monitor thread\n24/12/23 02:59:16 INFO YarnClientSchedulerBackend: Shutting down all executors\n24/12/23 02:59:16 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\n24/12/23 02:59:16 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped\n24/12/23 02:59:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n24/12/23 02:59:16 INFO MemoryStore: MemoryStore cleared\n24/12/23 02:59:16 INFO BlockManager: BlockManager stopped\n24/12/23 02:59:16 INFO BlockManagerMaster: BlockManagerMaster stopped\n24/12/23 02:59:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n24/12/23 02:59:16 INFO SparkContext: Successfully stopped SparkContext\n24/12/23 02:59:17 INFO ShutdownHookManager: Shutdown hook called\n24/12/23 02:59:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-761220f1-724e-40b1-91bf-ac0b50400b79\n24/12/23 02:59:17 INFO ShutdownHookManager: Deleting directory /usr/local/spark3.4.4/spark-63ecbba0-d7ea-4dee-bcb9-93c294e71d16/pyspark-482c8463-fba1-4930-b834-e85344a3bb4d\n24/12/23 02:59:17 INFO ShutdownHookManager: Deleting directory /usr/local/spark3.4.4/spark-63ecbba0-d7ea-4dee-bcb9-93c294e71d16\n"
}