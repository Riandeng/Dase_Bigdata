{
  "timestamp": "20241223_115204",
  "duration": 118.3966634273529,
  "success": true,
  "application_id": null,
  "stdout": "Results have been saved to q3_results_persist_MEMORY_ONLY.txt\nPerformance metrics have been saved to q3_performance_persist_MEMORY_ONLY.txt\n",
  "stderr": "24/12/23 11:52:07 INFO SparkContext: Running Spark version 3.4.4\n24/12/23 11:52:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n24/12/23 11:52:08 INFO ResourceUtils: ==============================================================\n24/12/23 11:52:08 INFO ResourceUtils: No custom resources configured for spark.driver.\n24/12/23 11:52:08 INFO ResourceUtils: ==============================================================\n24/12/23 11:52:08 INFO SparkContext: Submitted application: TPCH_Query3_persist_MEMORY_ONLY\n24/12/23 11:52:08 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 512, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n24/12/23 11:52:08 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n24/12/23 11:52:08 INFO ResourceProfileManager: Added ResourceProfile id: 0\n24/12/23 11:52:08 INFO SecurityManager: Changing view acls to: hdfs\n24/12/23 11:52:08 INFO SecurityManager: Changing modify acls to: hdfs\n24/12/23 11:52:08 INFO SecurityManager: Changing view acls groups to: \n24/12/23 11:52:08 INFO SecurityManager: Changing modify acls groups to: \n24/12/23 11:52:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hdfs; groups with view permissions: EMPTY; users with modify permissions: hdfs; groups with modify permissions: EMPTY\n24/12/23 11:52:08 INFO Utils: Successfully started service 'sparkDriver' on port 36393.\n24/12/23 11:52:09 INFO SparkEnv: Registering MapOutputTracker\n24/12/23 11:52:09 INFO SparkEnv: Registering BlockManagerMaster\n24/12/23 11:52:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n24/12/23 11:52:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n24/12/23 11:52:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n24/12/23 11:52:09 INFO DiskBlockManager: Created local directory at /usr/local/spark3.4.4/blockmgr-01f9bb76-ce18-420a-9be6-f2aa2bbb0ab3\n24/12/23 11:52:09 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n24/12/23 11:52:09 INFO SparkEnv: Registering OutputCommitCoordinator\n24/12/23 11:52:09 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n24/12/23 11:52:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n24/12/23 11:52:10 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at master/172.17.0.2:8032\n24/12/23 11:52:11 INFO Configuration: resource-types.xml not found\n24/12/23 11:52:11 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n24/12/23 11:52:11 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)\n24/12/23 11:52:11 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n24/12/23 11:52:11 INFO Client: Setting up container launch context for our AM\n24/12/23 11:52:11 INFO Client: Setting up the launch environment for our AM container\n24/12/23 11:52:11 INFO Client: Preparing resources for our AM container\n24/12/23 11:52:11 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n24/12/23 11:52:13 INFO Client: Uploading resource file:/usr/local/spark3.4.4/spark-8659bcb9-8f8a-4c96-990d-8fcc99ddd4f4/__spark_libs__4462326153766080868.zip -> hdfs://master:9000/user/hdfs/.sparkStaging/application_1734876530876_0014/__spark_libs__4462326153766080868.zip\n24/12/23 11:52:16 INFO Client: Uploading resource file:/usr/local/spark3.4.4/python/lib/pyspark.zip -> hdfs://master:9000/user/hdfs/.sparkStaging/application_1734876530876_0014/pyspark.zip\n24/12/23 11:52:16 INFO Client: Uploading resource file:/usr/local/spark3.4.4/python/lib/py4j-0.10.9.7-src.zip -> hdfs://master:9000/user/hdfs/.sparkStaging/application_1734876530876_0014/py4j-0.10.9.7-src.zip\n24/12/23 11:52:17 INFO Client: Uploading resource file:/usr/local/spark3.4.4/spark-8659bcb9-8f8a-4c96-990d-8fcc99ddd4f4/__spark_conf__466653561570423544.zip -> hdfs://master:9000/user/hdfs/.sparkStaging/application_1734876530876_0014/__spark_conf__.zip\n24/12/23 11:52:17 INFO SecurityManager: Changing view acls to: hdfs\n24/12/23 11:52:17 INFO SecurityManager: Changing modify acls to: hdfs\n24/12/23 11:52:17 INFO SecurityManager: Changing view acls groups to: \n24/12/23 11:52:17 INFO SecurityManager: Changing modify acls groups to: \n24/12/23 11:52:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hdfs; groups with view permissions: EMPTY; users with modify permissions: hdfs; groups with modify permissions: EMPTY\n24/12/23 11:52:17 INFO Client: Submitting application application_1734876530876_0014 to ResourceManager\n24/12/23 11:52:17 INFO YarnClientImpl: Submitted application application_1734876530876_0014\n24/12/23 11:52:18 INFO Client: Application report for application_1734876530876_0014 (state: ACCEPTED)\n24/12/23 11:52:18 INFO Client: \n\t client token: N/A\n\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n\t ApplicationMaster host: N/A\n\t ApplicationMaster RPC port: -1\n\t queue: root.default\n\t start time: 1734954737573\n\t final status: UNDEFINED\n\t tracking URL: http://master:8088/proxy/application_1734876530876_0014/\n\t user: hdfs\n24/12/23 11:52:19 INFO Client: Application report for application_1734876530876_0014 (state: ACCEPTED)\n24/12/23 11:52:20 INFO Client: Application report for application_1734876530876_0014 (state: ACCEPTED)\n24/12/23 11:52:21 INFO Client: Application report for application_1734876530876_0014 (state: ACCEPTED)\n24/12/23 11:52:22 INFO Client: Application report for application_1734876530876_0014 (state: ACCEPTED)\n24/12/23 11:52:23 INFO Client: Application report for application_1734876530876_0014 (state: ACCEPTED)\n24/12/23 11:52:24 INFO Client: Application report for application_1734876530876_0014 (state: RUNNING)\n24/12/23 11:52:24 INFO Client: \n\t client token: N/A\n\t diagnostics: N/A\n\t ApplicationMaster host: 172.17.0.4\n\t ApplicationMaster RPC port: -1\n\t queue: root.default\n\t start time: 1734954737573\n\t final status: UNDEFINED\n\t tracking URL: http://master:8088/proxy/application_1734876530876_0014/\n\t user: hdfs\n24/12/23 11:52:24 INFO YarnClientSchedulerBackend: Application application_1734876530876_0014 has started running.\n24/12/23 11:52:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36537.\n24/12/23 11:52:24 INFO NettyBlockTransferService: Server created on master:36537\n24/12/23 11:52:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n24/12/23 11:52:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, master, 36537, None)\n24/12/23 11:52:24 INFO BlockManagerMasterEndpoint: Registering block manager master:36537 with 366.3 MiB RAM, BlockManagerId(driver, master, 36537, None)\n24/12/23 11:52:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, master, 36537, None)\n24/12/23 11:52:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, master, 36537, None)\n24/12/23 11:52:24 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> master, PROXY_URI_BASES -> http://master:8088/proxy/application_1734876530876_0014), /proxy/application_1734876530876_0014\n24/12/23 11:52:25 INFO SingleEventLogFileWriter: Logging events to hdfs://master:9000/spark-history/application_1734876530876_0014.inprogress\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:25 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n24/12/23 11:52:32 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.0.4:57138) with ID 1,  ResourceProfileId 0\n24/12/23 11:52:32 INFO BlockManagerMasterEndpoint: Registering block manager slave02:34411 with 93.3 MiB RAM, BlockManagerId(1, slave02, 34411, None)\n24/12/23 11:52:35 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.0.3:54948) with ID 2,  ResourceProfileId 0\n24/12/23 11:52:35 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n24/12/23 11:52:35 INFO BlockManagerMasterEndpoint: Registering block manager slave01:38235 with 93.3 MiB RAM, BlockManagerId(2, slave01, 38235, None)\n24/12/23 11:52:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 364.8 KiB, free 365.9 MiB)\n24/12/23 11:52:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.0 KiB, free 365.9 MiB)\n24/12/23 11:52:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on master:36537 (size: 33.0 KiB, free: 366.3 MiB)\n24/12/23 11:52:35 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n24/12/23 11:52:35 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 364.8 KiB, free 365.6 MiB)\n24/12/23 11:52:35 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.0 KiB, free 365.5 MiB)\n24/12/23 11:52:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on master:36537 (size: 33.0 KiB, free: 366.2 MiB)\n24/12/23 11:52:35 INFO SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:0\n24/12/23 11:52:35 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 364.8 KiB, free 365.2 MiB)\n24/12/23 11:52:35 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.0 KiB, free 365.1 MiB)\n24/12/23 11:52:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on master:36537 (size: 33.0 KiB, free: 366.2 MiB)\n24/12/23 11:52:35 INFO SparkContext: Created broadcast 2 from textFile at NativeMethodAccessorImpl.java:0\n24/12/23 11:52:36 INFO FileInputFormat: Total input files to process : 1\n24/12/23 11:52:36 INFO FileInputFormat: Total input files to process : 1\n24/12/23 11:52:36 INFO NetworkTopology: Adding a new node: /default-rack/172.17.0.3:9866\n24/12/23 11:52:36 INFO NetworkTopology: Adding a new node: /default-rack/172.17.0.4:9866\n24/12/23 11:52:36 INFO FileInputFormat: Total input files to process : 1\n24/12/23 11:52:36 INFO SparkContext: Starting job: sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:67\n24/12/23 11:52:36 INFO DAGScheduler: Registering RDD 13 (join at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:50) as input to shuffle 2\n24/12/23 11:52:36 INFO DAGScheduler: Registering RDD 20 (join at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:54) as input to shuffle 1\n24/12/23 11:52:36 INFO DAGScheduler: Registering RDD 24 (reduceByKey at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:63) as input to shuffle 0\n24/12/23 11:52:36 INFO DAGScheduler: Got job 0 (sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:67) with 10 output partitions\n24/12/23 11:52:36 INFO DAGScheduler: Final stage: ResultStage 3 (sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:67)\n24/12/23 11:52:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n24/12/23 11:52:36 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)\n24/12/23 11:52:36 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[13] at join at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:50), which has no missing parents\n24/12/23 11:52:36 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 20.8 KiB, free 365.1 MiB)\n24/12/23 11:52:36 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.1 MiB)\n24/12/23 11:52:36 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on master:36537 (size: 9.0 KiB, free: 366.2 MiB)\n24/12/23 11:52:36 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1540\n24/12/23 11:52:36 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 0 (PairwiseRDD[13] at join at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:50) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n24/12/23 11:52:36 INFO YarnScheduler: Adding task set 0.0 with 4 tasks resource profile 0\n24/12/23 11:52:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (slave01, executor 2, partition 0, NODE_LOCAL, 8908 bytes) \n24/12/23 11:52:36 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (slave02, executor 1, partition 1, NODE_LOCAL, 8908 bytes) \n24/12/23 11:52:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on slave02:34411 (size: 9.0 KiB, free: 93.3 MiB)\n24/12/23 11:52:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on slave01:38235 (size: 9.0 KiB, free: 93.3 MiB)\n24/12/23 11:52:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on slave02:34411 (size: 33.0 KiB, free: 93.3 MiB)\n24/12/23 11:52:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on slave01:38235 (size: 33.0 KiB, free: 93.3 MiB)\n24/12/23 11:52:42 INFO BlockManagerInfo: Added rdd_6_1 in memory on slave02:34411 (size: 1495.1 KiB, free: 91.8 MiB)\n24/12/23 11:52:42 INFO BlockManagerInfo: Added rdd_6_0 in memory on slave01:38235 (size: 1492.7 KiB, free: 91.8 MiB)\n24/12/23 11:52:42 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (slave02, executor 1, partition 2, NODE_LOCAL, 8906 bytes) \n24/12/23 11:52:42 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 5941 ms on slave02 (executor 1) (1/4)\n24/12/23 11:52:42 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 60927\n24/12/23 11:52:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on slave02:34411 (size: 33.0 KiB, free: 91.8 MiB)\n24/12/23 11:52:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6388 ms on slave01 (executor 2) (2/4)\n24/12/23 11:52:45 INFO BlockManagerInfo: Added rdd_7_0 in memory on slave02:34411 (size: 6.3 MiB, free: 85.5 MiB)\n24/12/23 11:52:45 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (slave01, executor 2, partition 3, RACK_LOCAL, 8906 bytes) \n24/12/23 11:52:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on slave01:38235 (size: 33.0 KiB, free: 91.8 MiB)\n24/12/23 11:52:47 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 4984 ms on slave02 (executor 1) (3/4)\n24/12/23 11:52:49 INFO BlockManagerInfo: Added rdd_7_1 in memory on slave01:38235 (size: 6.2 MiB, free: 85.5 MiB)\n24/12/23 11:52:51 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 5334 ms on slave01 (executor 2) (4/4)\n24/12/23 11:52:51 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n24/12/23 11:52:51 INFO DAGScheduler: ShuffleMapStage 0 (join at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:50) finished in 14.547 s\n24/12/23 11:52:51 INFO DAGScheduler: looking for newly runnable stages\n24/12/23 11:52:51 INFO DAGScheduler: running: Set()\n24/12/23 11:52:51 INFO DAGScheduler: waiting: Set(ShuffleMapStage 1, ShuffleMapStage 2, ResultStage 3)\n24/12/23 11:52:51 INFO DAGScheduler: failed: Set()\n24/12/23 11:52:51 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[20] at join at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:54), which has no missing parents\n24/12/23 11:52:51 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 23.0 KiB, free 365.1 MiB)\n24/12/23 11:52:51 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.5 KiB, free 365.1 MiB)\n24/12/23 11:52:51 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on master:36537 (size: 10.5 KiB, free: 366.2 MiB)\n24/12/23 11:52:51 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1540\n24/12/23 11:52:51 INFO DAGScheduler: Submitting 10 missing tasks from ShuffleMapStage 1 (PairwiseRDD[20] at join at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:54) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n24/12/23 11:52:51 INFO YarnScheduler: Adding task set 1.0 with 10 tasks resource profile 0\n24/12/23 11:52:51 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4) (slave01, executor 2, partition 0, NODE_LOCAL, 8674 bytes) \n24/12/23 11:52:51 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5) (slave02, executor 1, partition 1, NODE_LOCAL, 8674 bytes) \n24/12/23 11:52:51 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on slave02:34411 (size: 10.5 KiB, free: 85.5 MiB)\n24/12/23 11:52:51 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on slave01:38235 (size: 10.5 KiB, free: 85.5 MiB)\n24/12/23 11:52:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.17.0.4:57138\n24/12/23 11:52:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.17.0.3:54948\n24/12/23 11:52:53 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6) (slave01, executor 2, partition 2, NODE_LOCAL, 8674 bytes) \n24/12/23 11:52:53 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 1711 ms on slave01 (executor 2) (1/10)\n24/12/23 11:52:53 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7) (slave02, executor 1, partition 3, NODE_LOCAL, 8674 bytes) \n24/12/23 11:52:53 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 1768 ms on slave02 (executor 1) (2/10)\n24/12/23 11:52:54 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 8) (slave01, executor 2, partition 4, NODE_LOCAL, 8908 bytes) \n24/12/23 11:52:54 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 1482 ms on slave01 (executor 2) (3/10)\n24/12/23 11:52:54 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 9) (slave02, executor 1, partition 5, NODE_LOCAL, 8908 bytes) \n24/12/23 11:52:54 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 1432 ms on slave02 (executor 1) (4/10)\n24/12/23 11:52:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on slave02:34411 (size: 33.0 KiB, free: 85.5 MiB)\n24/12/23 11:52:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on slave01:38235 (size: 33.0 KiB, free: 85.5 MiB)\n24/12/23 11:52:59 INFO BlockManagerInfo: Added rdd_8_0 in memory on slave01:38235 (size: 6.6 MiB, free: 78.9 MiB)\n24/12/23 11:52:59 INFO BlockManagerInfo: Added rdd_8_1 in memory on slave02:34411 (size: 6.7 MiB, free: 78.8 MiB)\n24/12/23 11:53:02 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 10) (slave01, executor 2, partition 6, NODE_LOCAL, 8908 bytes) \n24/12/23 11:53:02 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 8) in 8192 ms on slave01 (executor 2) (5/10)\n24/12/23 11:53:02 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 11) (slave02, executor 1, partition 7, NODE_LOCAL, 8908 bytes) \n24/12/23 11:53:02 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 9) in 8204 ms on slave02 (executor 1) (6/10)\n24/12/23 11:53:07 INFO BlockManagerInfo: Added rdd_8_2 in memory on slave01:38235 (size: 6.7 MiB, free: 72.2 MiB)\n24/12/23 11:53:08 INFO BlockManagerInfo: Added rdd_8_3 in memory on slave02:34411 (size: 6.7 MiB, free: 72.1 MiB)\n24/12/23 11:53:11 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 12) (slave01, executor 2, partition 8, NODE_LOCAL, 8908 bytes) \n24/12/23 11:53:11 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 10) in 8500 ms on slave01 (executor 2) (7/10)\n24/12/23 11:53:11 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 13) (slave02, executor 1, partition 9, NODE_LOCAL, 8908 bytes) \n24/12/23 11:53:11 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 11) in 8637 ms on slave02 (executor 1) (8/10)\n24/12/23 11:53:14 INFO BlockManagerInfo: Added rdd_8_5 in memory on slave02:34411 (size: 4.4 MiB, free: 67.7 MiB)\n24/12/23 11:53:15 INFO BlockManagerInfo: Added rdd_8_4 in memory on slave01:38235 (size: 6.7 MiB, free: 65.5 MiB)\n24/12/23 11:53:16 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 13) in 5428 ms on slave02 (executor 1) (9/10)\n24/12/23 11:53:18 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 12) in 7338 ms on slave01 (executor 2) (10/10)\n24/12/23 11:53:18 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \n24/12/23 11:53:18 INFO DAGScheduler: ShuffleMapStage 1 (join at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:54) finished in 27.226 s\n24/12/23 11:53:18 INFO DAGScheduler: looking for newly runnable stages\n24/12/23 11:53:18 INFO DAGScheduler: running: Set()\n24/12/23 11:53:18 INFO DAGScheduler: waiting: Set(ShuffleMapStage 2, ResultStage 3)\n24/12/23 11:53:18 INFO DAGScheduler: failed: Set()\n24/12/23 11:53:18 INFO DAGScheduler: Submitting ShuffleMapStage 2 (PairwiseRDD[24] at reduceByKey at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:63), which has no missing parents\n24/12/23 11:53:18 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 14.7 KiB, free 365.1 MiB)\n24/12/23 11:53:18 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 365.1 MiB)\n24/12/23 11:53:18 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on master:36537 (size: 8.3 KiB, free: 366.2 MiB)\n24/12/23 11:53:18 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1540\n24/12/23 11:53:18 INFO DAGScheduler: Submitting 10 missing tasks from ShuffleMapStage 2 (PairwiseRDD[24] at reduceByKey at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:63) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n24/12/23 11:53:18 INFO YarnScheduler: Adding task set 2.0 with 10 tasks resource profile 0\n24/12/23 11:53:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 14) (slave02, executor 1, partition 0, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:18 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 15) (slave01, executor 2, partition 1, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:18 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on slave01:38235 (size: 8.3 KiB, free: 65.5 MiB)\n24/12/23 11:53:18 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on slave02:34411 (size: 8.3 KiB, free: 67.7 MiB)\n24/12/23 11:53:18 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.17.0.3:54948\n24/12/23 11:53:18 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.17.0.4:57138\n24/12/23 11:53:19 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 16) (slave01, executor 2, partition 2, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:19 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 15) in 1195 ms on slave01 (executor 2) (1/10)\n24/12/23 11:53:19 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 17) (slave02, executor 1, partition 3, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 14) in 1340 ms on slave02 (executor 1) (2/10)\n24/12/23 11:53:21 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 18) (slave01, executor 2, partition 4, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:21 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 16) in 1277 ms on slave01 (executor 2) (3/10)\n24/12/23 11:53:21 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 19) (slave02, executor 1, partition 5, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:21 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 17) in 1251 ms on slave02 (executor 1) (4/10)\n24/12/23 11:53:22 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 20) (slave01, executor 2, partition 6, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:22 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 18) in 1183 ms on slave01 (executor 2) (5/10)\n24/12/23 11:53:22 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 21) (slave02, executor 1, partition 7, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:22 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 19) in 1161 ms on slave02 (executor 1) (6/10)\n24/12/23 11:53:23 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 22) (slave01, executor 2, partition 8, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:23 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 20) in 1218 ms on slave01 (executor 2) (7/10)\n24/12/23 11:53:23 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 23) (slave02, executor 1, partition 9, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:23 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 21) in 1237 ms on slave02 (executor 1) (8/10)\n24/12/23 11:53:24 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 22) in 1151 ms on slave01 (executor 2) (9/10)\n24/12/23 11:53:24 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 23) in 1158 ms on slave02 (executor 1) (10/10)\n24/12/23 11:53:24 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \n24/12/23 11:53:24 INFO DAGScheduler: ShuffleMapStage 2 (reduceByKey at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:63) finished in 6.147 s\n24/12/23 11:53:24 INFO DAGScheduler: looking for newly runnable stages\n24/12/23 11:53:24 INFO DAGScheduler: running: Set()\n24/12/23 11:53:24 INFO DAGScheduler: waiting: Set(ResultStage 3)\n24/12/23 11:53:24 INFO DAGScheduler: failed: Set()\n24/12/23 11:53:24 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[27] at sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:67), which has no missing parents\n24/12/23 11:53:24 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 12.2 KiB, free 365.0 MiB)\n24/12/23 11:53:24 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 365.0 MiB)\n24/12/23 11:53:24 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on master:36537 (size: 6.7 KiB, free: 366.2 MiB)\n24/12/23 11:53:24 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1540\n24/12/23 11:53:24 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 3 (PythonRDD[27] at sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:67) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n24/12/23 11:53:24 INFO YarnScheduler: Adding task set 3.0 with 10 tasks resource profile 0\n24/12/23 11:53:24 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 24) (slave02, executor 1, partition 0, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:24 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 25) (slave01, executor 2, partition 1, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:24 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on slave02:34411 (size: 6.7 KiB, free: 67.7 MiB)\n24/12/23 11:53:24 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on slave01:38235 (size: 6.7 KiB, free: 65.5 MiB)\n24/12/23 11:53:24 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.17.0.3:54948\n24/12/23 11:53:24 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.17.0.4:57138\n24/12/23 11:53:24 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 26) (slave02, executor 1, partition 2, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:24 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 24) in 233 ms on slave02 (executor 1) (1/10)\n24/12/23 11:53:24 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 27) (slave01, executor 2, partition 3, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:24 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 25) in 265 ms on slave01 (executor 2) (2/10)\n24/12/23 11:53:25 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 28) (slave02, executor 1, partition 4, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:25 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 26) in 230 ms on slave02 (executor 1) (3/10)\n24/12/23 11:53:25 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 29) (slave01, executor 2, partition 5, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:25 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 27) in 240 ms on slave01 (executor 2) (4/10)\n24/12/23 11:53:25 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 30) (slave02, executor 1, partition 6, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:25 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 31) (slave01, executor 2, partition 7, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:25 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 28) in 282 ms on slave02 (executor 1) (5/10)\n24/12/23 11:53:25 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 29) in 255 ms on slave01 (executor 2) (6/10)\n24/12/23 11:53:25 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 32) (slave01, executor 2, partition 8, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:25 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 31) in 219 ms on slave01 (executor 2) (7/10)\n24/12/23 11:53:25 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 33) (slave02, executor 1, partition 9, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:25 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 30) in 253 ms on slave02 (executor 1) (8/10)\n24/12/23 11:53:25 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 32) in 201 ms on slave01 (executor 2) (9/10)\n24/12/23 11:53:25 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 33) in 198 ms on slave02 (executor 1) (10/10)\n24/12/23 11:53:25 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \n24/12/23 11:53:25 INFO DAGScheduler: ResultStage 3 (sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:67) finished in 1.197 s\n24/12/23 11:53:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n24/12/23 11:53:25 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished\n24/12/23 11:53:25 INFO DAGScheduler: Job 0 finished: sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:67, took 49.375463 s\n24/12/23 11:53:25 INFO SparkContext: Starting job: sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:67\n24/12/23 11:53:25 INFO DAGScheduler: Got job 1 (sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:67) with 10 output partitions\n24/12/23 11:53:25 INFO DAGScheduler: Final stage: ResultStage 7 (sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:67)\n24/12/23 11:53:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n24/12/23 11:53:25 INFO DAGScheduler: Missing parents: List()\n24/12/23 11:53:25 INFO DAGScheduler: Submitting ResultStage 7 (PythonRDD[28] at sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:67), which has no missing parents\n24/12/23 11:53:25 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 11.5 KiB, free 365.0 MiB)\n24/12/23 11:53:25 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.0 MiB)\n24/12/23 11:53:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on master:36537 (size: 6.5 KiB, free: 366.2 MiB)\n24/12/23 11:53:26 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1540\n24/12/23 11:53:26 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (PythonRDD[28] at sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:67) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n24/12/23 11:53:26 INFO YarnScheduler: Adding task set 7.0 with 10 tasks resource profile 0\n24/12/23 11:53:26 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 34) (slave01, executor 2, partition 0, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:26 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 35) (slave02, executor 1, partition 1, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on slave01:38235 (size: 6.5 KiB, free: 65.5 MiB)\n24/12/23 11:53:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on slave02:34411 (size: 6.5 KiB, free: 67.7 MiB)\n24/12/23 11:53:26 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 36) (slave02, executor 1, partition 2, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:26 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 35) in 278 ms on slave02 (executor 1) (1/10)\n24/12/23 11:53:26 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 37) (slave01, executor 2, partition 3, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:26 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 34) in 303 ms on slave01 (executor 2) (2/10)\n24/12/23 11:53:26 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 38) (slave02, executor 1, partition 4, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:26 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 36) in 248 ms on slave02 (executor 1) (3/10)\n24/12/23 11:53:26 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 39) (slave01, executor 2, partition 5, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:26 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 37) in 256 ms on slave01 (executor 2) (4/10)\n24/12/23 11:53:26 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 40) (slave01, executor 2, partition 6, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:26 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 39) in 214 ms on slave01 (executor 2) (5/10)\n24/12/23 11:53:26 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 41) (slave02, executor 1, partition 7, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:26 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 38) in 252 ms on slave02 (executor 1) (6/10)\n24/12/23 11:53:26 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 42) (slave01, executor 2, partition 8, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:26 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 40) in 196 ms on slave01 (executor 2) (7/10)\n24/12/23 11:53:26 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 41) in 214 ms on slave02 (executor 1) (8/10)\n24/12/23 11:53:26 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 43) (slave02, executor 1, partition 9, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:27 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 42) in 188 ms on slave01 (executor 2) (9/10)\n24/12/23 11:53:27 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 43) in 189 ms on slave02 (executor 1) (10/10)\n24/12/23 11:53:27 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \n24/12/23 11:53:27 INFO DAGScheduler: ResultStage 7 (sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:67) finished in 1.193 s\n24/12/23 11:53:27 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n24/12/23 11:53:27 INFO YarnScheduler: Killing all running tasks in stage 7: Stage finished\n24/12/23 11:53:27 INFO DAGScheduler: Job 1 finished: sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:67, took 1.205211 s\n24/12/23 11:53:27 INFO SparkContext: Starting job: collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:77\n24/12/23 11:53:27 INFO DAGScheduler: Registering RDD 30 (sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:67) as input to shuffle 3\n24/12/23 11:53:27 INFO DAGScheduler: Got job 2 (collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:77) with 10 output partitions\n24/12/23 11:53:27 INFO DAGScheduler: Final stage: ResultStage 12 (collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:77)\n24/12/23 11:53:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n24/12/23 11:53:27 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 11)\n24/12/23 11:53:27 INFO DAGScheduler: Submitting ShuffleMapStage 11 (PairwiseRDD[30] at sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:67), which has no missing parents\n24/12/23 11:53:27 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 12.5 KiB, free 365.0 MiB)\n24/12/23 11:53:27 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 365.0 MiB)\n24/12/23 11:53:27 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on master:36537 (size: 7.5 KiB, free: 366.2 MiB)\n24/12/23 11:53:27 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1540\n24/12/23 11:53:27 INFO DAGScheduler: Submitting 10 missing tasks from ShuffleMapStage 11 (PairwiseRDD[30] at sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:67) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n24/12/23 11:53:27 INFO YarnScheduler: Adding task set 11.0 with 10 tasks resource profile 0\n24/12/23 11:53:27 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 44) (slave01, executor 2, partition 0, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:27 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 45) (slave02, executor 1, partition 1, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:27 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on slave02:34411 (size: 7.5 KiB, free: 67.7 MiB)\n24/12/23 11:53:27 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on slave01:38235 (size: 7.5 KiB, free: 65.5 MiB)\n24/12/23 11:53:27 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 46) (slave02, executor 1, partition 2, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:27 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 45) in 304 ms on slave02 (executor 1) (1/10)\n24/12/23 11:53:27 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 47) (slave01, executor 2, partition 3, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:27 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 44) in 430 ms on slave01 (executor 2) (2/10)\n24/12/23 11:53:27 INFO TaskSetManager: Starting task 4.0 in stage 11.0 (TID 48) (slave02, executor 1, partition 4, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:27 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 46) in 423 ms on slave02 (executor 1) (3/10)\n24/12/23 11:53:28 INFO TaskSetManager: Starting task 5.0 in stage 11.0 (TID 49) (slave01, executor 2, partition 5, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:28 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 47) in 333 ms on slave01 (executor 2) (4/10)\n24/12/23 11:53:28 INFO TaskSetManager: Starting task 6.0 in stage 11.0 (TID 50) (slave01, executor 2, partition 6, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:28 INFO TaskSetManager: Finished task 5.0 in stage 11.0 (TID 49) in 237 ms on slave01 (executor 2) (5/10)\n24/12/23 11:53:28 INFO TaskSetManager: Starting task 7.0 in stage 11.0 (TID 51) (slave02, executor 1, partition 7, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:28 INFO TaskSetManager: Finished task 4.0 in stage 11.0 (TID 48) in 287 ms on slave02 (executor 1) (6/10)\n24/12/23 11:53:28 INFO TaskSetManager: Starting task 8.0 in stage 11.0 (TID 52) (slave01, executor 2, partition 8, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:28 INFO TaskSetManager: Starting task 9.0 in stage 11.0 (TID 53) (slave02, executor 1, partition 9, NODE_LOCAL, 8565 bytes) \n24/12/23 11:53:28 INFO TaskSetManager: Finished task 6.0 in stage 11.0 (TID 50) in 231 ms on slave01 (executor 2) (7/10)\n24/12/23 11:53:28 INFO TaskSetManager: Finished task 7.0 in stage 11.0 (TID 51) in 219 ms on slave02 (executor 1) (8/10)\n24/12/23 11:53:28 INFO TaskSetManager: Finished task 8.0 in stage 11.0 (TID 52) in 220 ms on slave01 (executor 2) (9/10)\n24/12/23 11:53:28 INFO TaskSetManager: Finished task 9.0 in stage 11.0 (TID 53) in 224 ms on slave02 (executor 1) (10/10)\n24/12/23 11:53:28 INFO YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \n24/12/23 11:53:28 INFO DAGScheduler: ShuffleMapStage 11 (sortBy at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:67) finished in 1.453 s\n24/12/23 11:53:28 INFO DAGScheduler: looking for newly runnable stages\n24/12/23 11:53:28 INFO DAGScheduler: running: Set()\n24/12/23 11:53:28 INFO DAGScheduler: waiting: Set(ResultStage 12)\n24/12/23 11:53:28 INFO DAGScheduler: failed: Set()\n24/12/23 11:53:28 INFO DAGScheduler: Submitting ResultStage 12 (PythonRDD[33] at collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:77), which has no missing parents\n24/12/23 11:53:28 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 10.0 KiB, free 365.0 MiB)\n24/12/23 11:53:28 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 365.0 MiB)\n24/12/23 11:53:28 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on master:36537 (size: 5.9 KiB, free: 366.2 MiB)\n24/12/23 11:53:28 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1540\n24/12/23 11:53:28 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 12 (PythonRDD[33] at collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:77) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n24/12/23 11:53:28 INFO YarnScheduler: Adding task set 12.0 with 10 tasks resource profile 0\n24/12/23 11:53:28 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 54) (slave02, executor 1, partition 0, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:28 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 55) (slave01, executor 2, partition 1, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:28 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on slave02:34411 (size: 5.9 KiB, free: 67.7 MiB)\n24/12/23 11:53:28 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on slave01:38235 (size: 5.9 KiB, free: 65.5 MiB)\n24/12/23 11:53:28 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.17.0.4:57138\n24/12/23 11:53:28 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.17.0.3:54948\n24/12/23 11:53:28 INFO TaskSetManager: Starting task 2.0 in stage 12.0 (TID 56) (slave01, executor 2, partition 2, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:28 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 55) in 228 ms on slave01 (executor 2) (1/10)\n24/12/23 11:53:29 INFO TaskSetManager: Starting task 3.0 in stage 12.0 (TID 57) (slave02, executor 1, partition 3, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:29 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 54) in 271 ms on slave02 (executor 1) (2/10)\n24/12/23 11:53:29 INFO TaskSetManager: Starting task 4.0 in stage 12.0 (TID 58) (slave01, executor 2, partition 4, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:29 INFO TaskSetManager: Finished task 2.0 in stage 12.0 (TID 56) in 196 ms on slave01 (executor 2) (3/10)\n24/12/23 11:53:29 INFO TaskSetManager: Starting task 5.0 in stage 12.0 (TID 59) (slave02, executor 1, partition 5, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:29 INFO TaskSetManager: Finished task 3.0 in stage 12.0 (TID 57) in 200 ms on slave02 (executor 1) (4/10)\n24/12/23 11:53:29 INFO TaskSetManager: Starting task 6.0 in stage 12.0 (TID 60) (slave01, executor 2, partition 6, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:29 INFO TaskSetManager: Finished task 4.0 in stage 12.0 (TID 58) in 167 ms on slave01 (executor 2) (5/10)\n24/12/23 11:53:29 INFO TaskSetManager: Starting task 7.0 in stage 12.0 (TID 61) (slave02, executor 1, partition 7, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:29 INFO TaskSetManager: Finished task 5.0 in stage 12.0 (TID 59) in 193 ms on slave02 (executor 1) (6/10)\n24/12/23 11:53:29 INFO TaskSetManager: Starting task 8.0 in stage 12.0 (TID 62) (slave01, executor 2, partition 8, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:29 INFO TaskSetManager: Finished task 6.0 in stage 12.0 (TID 60) in 179 ms on slave01 (executor 2) (7/10)\n24/12/23 11:53:29 INFO TaskSetManager: Starting task 9.0 in stage 12.0 (TID 63) (slave02, executor 1, partition 9, NODE_LOCAL, 8576 bytes) \n24/12/23 11:53:29 INFO TaskSetManager: Finished task 7.0 in stage 12.0 (TID 61) in 190 ms on slave02 (executor 1) (8/10)\n24/12/23 11:53:29 INFO TaskSetManager: Finished task 8.0 in stage 12.0 (TID 62) in 227 ms on slave01 (executor 2) (9/10)\n24/12/23 11:53:29 INFO TaskSetManager: Finished task 9.0 in stage 12.0 (TID 63) in 231 ms on slave02 (executor 1) (10/10)\n24/12/23 11:53:29 INFO YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool \n24/12/23 11:53:29 INFO DAGScheduler: ResultStage 12 (collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:77) finished in 1.101 s\n24/12/23 11:53:29 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n24/12/23 11:53:29 INFO YarnScheduler: Killing all running tasks in stage 12: Stage finished\n24/12/23 11:53:29 INFO DAGScheduler: Job 2 finished: collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:77, took 2.579280 s\n24/12/23 11:53:29 INFO SparkContext: Starting job: sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9\n24/12/23 11:53:29 INFO DAGScheduler: Got job 3 (sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9) with 2 output partitions\n24/12/23 11:53:29 INFO DAGScheduler: Final stage: ResultStage 13 (sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9)\n24/12/23 11:53:29 INFO DAGScheduler: Parents of final stage: List()\n24/12/23 11:53:29 INFO DAGScheduler: Missing parents: List()\n24/12/23 11:53:29 INFO DAGScheduler: Submitting ResultStage 13 (PythonRDD[34] at sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9), which has no missing parents\n24/12/23 11:53:29 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 9.5 KiB, free 365.0 MiB)\n24/12/23 11:53:29 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 365.0 MiB)\n24/12/23 11:53:29 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on master:36537 (size: 5.6 KiB, free: 366.1 MiB)\n24/12/23 11:53:29 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1540\n24/12/23 11:53:29 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 13 (PythonRDD[34] at sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9) (first 15 tasks are for partitions Vector(0, 1))\n24/12/23 11:53:29 INFO YarnScheduler: Adding task set 13.0 with 2 tasks resource profile 0\n24/12/23 11:53:29 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 64) (slave02, executor 1, partition 0, NODE_LOCAL, 8810 bytes) \n24/12/23 11:53:29 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 65) (slave01, executor 2, partition 1, NODE_LOCAL, 8810 bytes) \n24/12/23 11:53:29 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on slave01:38235 (size: 5.6 KiB, free: 65.5 MiB)\n24/12/23 11:53:29 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on slave02:34411 (size: 5.6 KiB, free: 67.7 MiB)\n24/12/23 11:53:30 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 65) in 761 ms on slave01 (executor 2) (1/2)\n24/12/23 11:53:30 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 64) in 810 ms on slave02 (executor 1) (2/2)\n24/12/23 11:53:30 INFO YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \n24/12/23 11:53:30 INFO DAGScheduler: ResultStage 13 (sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9) finished in 0.832 s\n24/12/23 11:53:30 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n24/12/23 11:53:30 INFO YarnScheduler: Killing all running tasks in stage 13: Stage finished\n24/12/23 11:53:30 INFO DAGScheduler: Job 3 finished: sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9, took 0.847308 s\n24/12/23 11:53:30 INFO SparkContext: Starting job: sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9\n24/12/23 11:53:30 INFO DAGScheduler: Got job 4 (sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9) with 2 output partitions\n24/12/23 11:53:30 INFO DAGScheduler: Final stage: ResultStage 14 (sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9)\n24/12/23 11:53:30 INFO DAGScheduler: Parents of final stage: List()\n24/12/23 11:53:30 INFO DAGScheduler: Missing parents: List()\n24/12/23 11:53:30 INFO DAGScheduler: Submitting ResultStage 14 (PythonRDD[35] at sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9), which has no missing parents\n24/12/23 11:53:30 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 9.5 KiB, free 365.0 MiB)\n24/12/23 11:53:30 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 364.9 MiB)\n24/12/23 11:53:30 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on master:36537 (size: 5.6 KiB, free: 366.1 MiB)\n24/12/23 11:53:30 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1540\n24/12/23 11:53:30 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 14 (PythonRDD[35] at sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9) (first 15 tasks are for partitions Vector(0, 1))\n24/12/23 11:53:30 INFO YarnScheduler: Adding task set 14.0 with 2 tasks resource profile 0\n24/12/23 11:53:30 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 66) (slave02, executor 1, partition 0, NODE_LOCAL, 8808 bytes) \n24/12/23 11:53:30 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on slave02:34411 (size: 5.6 KiB, free: 67.7 MiB)\n24/12/23 11:53:30 INFO BlockManagerInfo: Removed broadcast_5_piece0 on master:36537 in memory (size: 8.3 KiB, free: 366.1 MiB)\n24/12/23 11:53:30 INFO BlockManagerInfo: Removed broadcast_5_piece0 on slave01:38235 in memory (size: 8.3 KiB, free: 65.5 MiB)\n24/12/23 11:53:30 INFO BlockManagerInfo: Removed broadcast_5_piece0 on slave02:34411 in memory (size: 8.3 KiB, free: 67.7 MiB)\n24/12/23 11:53:30 INFO BlockManagerInfo: Removed broadcast_3_piece0 on slave01:38235 in memory (size: 9.0 KiB, free: 65.5 MiB)\n24/12/23 11:53:30 INFO BlockManagerInfo: Removed broadcast_3_piece0 on master:36537 in memory (size: 9.0 KiB, free: 366.2 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_3_piece0 on slave02:34411 in memory (size: 9.0 KiB, free: 67.7 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_8_piece0 on master:36537 in memory (size: 7.5 KiB, free: 366.2 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_8_piece0 on slave01:38235 in memory (size: 7.5 KiB, free: 65.5 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_8_piece0 on slave02:34411 in memory (size: 7.5 KiB, free: 67.7 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_10_piece0 on master:36537 in memory (size: 5.6 KiB, free: 366.2 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_10_piece0 on slave01:38235 in memory (size: 5.6 KiB, free: 65.5 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_10_piece0 on slave02:34411 in memory (size: 5.6 KiB, free: 67.7 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_9_piece0 on master:36537 in memory (size: 5.9 KiB, free: 366.2 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_9_piece0 on slave01:38235 in memory (size: 5.9 KiB, free: 65.5 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_9_piece0 on slave02:34411 in memory (size: 5.9 KiB, free: 67.7 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_6_piece0 on master:36537 in memory (size: 6.7 KiB, free: 366.2 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_6_piece0 on slave01:38235 in memory (size: 6.7 KiB, free: 65.5 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_6_piece0 on slave02:34411 in memory (size: 6.7 KiB, free: 67.7 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_4_piece0 on master:36537 in memory (size: 10.5 KiB, free: 366.2 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_4_piece0 on slave02:34411 in memory (size: 10.5 KiB, free: 67.7 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_4_piece0 on slave01:38235 in memory (size: 10.5 KiB, free: 65.6 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_7_piece0 on master:36537 in memory (size: 6.5 KiB, free: 366.2 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_7_piece0 on slave02:34411 in memory (size: 6.5 KiB, free: 67.7 MiB)\n24/12/23 11:53:31 INFO BlockManagerInfo: Removed broadcast_7_piece0 on slave01:38235 in memory (size: 6.5 KiB, free: 65.6 MiB)\n24/12/23 11:53:33 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 67) (slave01, executor 2, partition 1, RACK_LOCAL, 8808 bytes) \n24/12/23 11:53:33 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on slave01:38235 (size: 5.6 KiB, free: 65.6 MiB)\n24/12/23 11:53:34 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 66) in 3221 ms on slave02 (executor 1) (1/2)\n24/12/23 11:53:37 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 67) in 3426 ms on slave01 (executor 2) (2/2)\n24/12/23 11:53:37 INFO YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \n24/12/23 11:53:37 INFO DAGScheduler: ResultStage 14 (sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9) finished in 6.607 s\n24/12/23 11:53:37 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n24/12/23 11:53:37 INFO YarnScheduler: Killing all running tasks in stage 14: Stage finished\n24/12/23 11:53:37 INFO DAGScheduler: Job 4 finished: sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9, took 6.613454 s\n24/12/23 11:53:37 INFO SparkContext: Starting job: sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9\n24/12/23 11:53:37 INFO DAGScheduler: Got job 5 (sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9) with 6 output partitions\n24/12/23 11:53:37 INFO DAGScheduler: Final stage: ResultStage 15 (sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9)\n24/12/23 11:53:37 INFO DAGScheduler: Parents of final stage: List()\n24/12/23 11:53:37 INFO DAGScheduler: Missing parents: List()\n24/12/23 11:53:37 INFO DAGScheduler: Submitting ResultStage 15 (PythonRDD[36] at sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9), which has no missing parents\n24/12/23 11:53:37 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 9.5 KiB, free 365.1 MiB)\n24/12/23 11:53:37 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 365.1 MiB)\n24/12/23 11:53:37 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on master:36537 (size: 5.6 KiB, free: 366.2 MiB)\n24/12/23 11:53:37 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1540\n24/12/23 11:53:37 INFO DAGScheduler: Submitting 6 missing tasks from ResultStage 15 (PythonRDD[36] at sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n24/12/23 11:53:37 INFO YarnScheduler: Adding task set 15.0 with 6 tasks resource profile 0\n24/12/23 11:53:37 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 68) (slave02, executor 1, partition 0, NODE_LOCAL, 8810 bytes) \n24/12/23 11:53:37 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 69) (slave01, executor 2, partition 1, NODE_LOCAL, 8810 bytes) \n24/12/23 11:53:37 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on slave01:38235 (size: 5.6 KiB, free: 65.5 MiB)\n24/12/23 11:53:37 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on slave02:34411 (size: 5.6 KiB, free: 67.7 MiB)\n24/12/23 11:53:45 INFO TaskSetManager: Starting task 2.0 in stage 15.0 (TID 70) (slave01, executor 2, partition 2, NODE_LOCAL, 8810 bytes) \n24/12/23 11:53:45 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 69) in 7853 ms on slave01 (executor 2) (1/6)\n24/12/23 11:53:45 INFO TaskSetManager: Starting task 3.0 in stage 15.0 (TID 71) (slave02, executor 1, partition 3, NODE_LOCAL, 8810 bytes) \n24/12/23 11:53:45 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 68) in 7969 ms on slave02 (executor 1) (2/6)\n24/12/23 11:53:53 INFO TaskSetManager: Starting task 4.0 in stage 15.0 (TID 72) (slave01, executor 2, partition 4, NODE_LOCAL, 8810 bytes) \n24/12/23 11:53:53 INFO TaskSetManager: Finished task 2.0 in stage 15.0 (TID 70) in 7781 ms on slave01 (executor 2) (3/6)\n24/12/23 11:53:53 INFO TaskSetManager: Starting task 5.0 in stage 15.0 (TID 73) (slave02, executor 1, partition 5, NODE_LOCAL, 8810 bytes) \n24/12/23 11:53:53 INFO TaskSetManager: Finished task 3.0 in stage 15.0 (TID 71) in 7728 ms on slave02 (executor 1) (4/6)\n24/12/23 11:53:57 INFO TaskSetManager: Finished task 5.0 in stage 15.0 (TID 73) in 4697 ms on slave02 (executor 1) (5/6)\n24/12/23 11:53:59 INFO TaskSetManager: Finished task 4.0 in stage 15.0 (TID 72) in 6777 ms on slave01 (executor 2) (6/6)\n24/12/23 11:53:59 INFO YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \n24/12/23 11:53:59 INFO DAGScheduler: ResultStage 15 (sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9) finished in 22.421 s\n24/12/23 11:53:59 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n24/12/23 11:53:59 INFO YarnScheduler: Killing all running tasks in stage 15: Stage finished\n24/12/23 11:53:59 INFO DAGScheduler: Job 5 finished: sum at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:9, took 22.431105 s\n24/12/23 11:53:59 INFO SparkContext: Starting job: collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12\n24/12/23 11:53:59 INFO DAGScheduler: Got job 6 (collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12) with 2 output partitions\n24/12/23 11:53:59 INFO DAGScheduler: Final stage: ResultStage 16 (collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12)\n24/12/23 11:53:59 INFO DAGScheduler: Parents of final stage: List()\n24/12/23 11:53:59 INFO DAGScheduler: Missing parents: List()\n24/12/23 11:53:59 INFO DAGScheduler: Submitting ResultStage 16 (PythonRDD[37] at collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12), which has no missing parents\n24/12/23 11:53:59 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 10.2 KiB, free 365.1 MiB)\n24/12/23 11:53:59 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 365.1 MiB)\n24/12/23 11:53:59 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on master:36537 (size: 5.6 KiB, free: 366.2 MiB)\n24/12/23 11:53:59 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1540\n24/12/23 11:53:59 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 16 (PythonRDD[37] at collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12) (first 15 tasks are for partitions Vector(0, 1))\n24/12/23 11:53:59 INFO YarnScheduler: Adding task set 16.0 with 2 tasks resource profile 0\n24/12/23 11:53:59 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 74) (slave01, executor 2, partition 0, PROCESS_LOCAL, 8810 bytes) \n24/12/23 11:53:59 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 75) (slave02, executor 1, partition 1, PROCESS_LOCAL, 8810 bytes) \n24/12/23 11:53:59 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on slave01:38235 (size: 5.6 KiB, free: 65.5 MiB)\n24/12/23 11:53:59 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on slave02:34411 (size: 5.6 KiB, free: 67.7 MiB)\n24/12/23 11:54:00 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 74) in 198 ms on slave01 (executor 2) (1/2)\n24/12/23 11:54:00 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 75) in 205 ms on slave02 (executor 1) (2/2)\n24/12/23 11:54:00 INFO YarnScheduler: Removed TaskSet 16.0, whose tasks have all completed, from pool \n24/12/23 11:54:00 INFO DAGScheduler: ResultStage 16 (collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12) finished in 0.222 s\n24/12/23 11:54:00 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n24/12/23 11:54:00 INFO YarnScheduler: Killing all running tasks in stage 16: Stage finished\n24/12/23 11:54:00 INFO DAGScheduler: Job 6 finished: collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12, took 0.234302 s\n24/12/23 11:54:00 INFO SparkContext: Starting job: collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12\n24/12/23 11:54:00 INFO DAGScheduler: Got job 7 (collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12) with 2 output partitions\n24/12/23 11:54:00 INFO DAGScheduler: Final stage: ResultStage 17 (collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12)\n24/12/23 11:54:00 INFO DAGScheduler: Parents of final stage: List()\n24/12/23 11:54:00 INFO DAGScheduler: Missing parents: List()\n24/12/23 11:54:00 INFO DAGScheduler: Submitting ResultStage 17 (PythonRDD[38] at collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12), which has no missing parents\n24/12/23 11:54:00 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 10.2 KiB, free 365.1 MiB)\n24/12/23 11:54:00 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 365.1 MiB)\n24/12/23 11:54:00 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on master:36537 (size: 5.6 KiB, free: 366.2 MiB)\n24/12/23 11:54:00 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1540\n24/12/23 11:54:00 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 17 (PythonRDD[38] at collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12) (first 15 tasks are for partitions Vector(0, 1))\n24/12/23 11:54:00 INFO YarnScheduler: Adding task set 17.0 with 2 tasks resource profile 0\n24/12/23 11:54:00 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 76) (slave01, executor 2, partition 1, PROCESS_LOCAL, 8808 bytes) \n24/12/23 11:54:00 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 77) (slave02, executor 1, partition 0, PROCESS_LOCAL, 8808 bytes) \n24/12/23 11:54:00 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on slave02:34411 (size: 5.6 KiB, free: 67.7 MiB)\n24/12/23 11:54:00 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on slave01:38235 (size: 5.6 KiB, free: 65.5 MiB)\n24/12/23 11:54:00 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 77) in 495 ms on slave02 (executor 1) (1/2)\n24/12/23 11:54:00 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 76) in 497 ms on slave01 (executor 2) (2/2)\n24/12/23 11:54:00 INFO YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \n24/12/23 11:54:00 INFO DAGScheduler: ResultStage 17 (collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12) finished in 0.513 s\n24/12/23 11:54:00 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n24/12/23 11:54:00 INFO YarnScheduler: Killing all running tasks in stage 17: Stage finished\n24/12/23 11:54:00 INFO DAGScheduler: Job 7 finished: collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12, took 0.527323 s\n24/12/23 11:54:00 INFO SparkContext: Starting job: collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12\n24/12/23 11:54:00 INFO DAGScheduler: Got job 8 (collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12) with 6 output partitions\n24/12/23 11:54:00 INFO DAGScheduler: Final stage: ResultStage 18 (collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12)\n24/12/23 11:54:00 INFO DAGScheduler: Parents of final stage: List()\n24/12/23 11:54:00 INFO DAGScheduler: Missing parents: List()\n24/12/23 11:54:00 INFO DAGScheduler: Submitting ResultStage 18 (PythonRDD[39] at collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12), which has no missing parents\n24/12/23 11:54:00 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 10.2 KiB, free 365.1 MiB)\n24/12/23 11:54:00 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 365.1 MiB)\n24/12/23 11:54:00 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on master:36537 (size: 5.6 KiB, free: 366.2 MiB)\n24/12/23 11:54:00 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1540\n24/12/23 11:54:00 INFO DAGScheduler: Submitting 6 missing tasks from ResultStage 18 (PythonRDD[39] at collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n24/12/23 11:54:00 INFO YarnScheduler: Adding task set 18.0 with 6 tasks resource profile 0\n24/12/23 11:54:00 INFO TaskSetManager: Starting task 1.0 in stage 18.0 (TID 78) (slave02, executor 1, partition 1, PROCESS_LOCAL, 8810 bytes) \n24/12/23 11:54:00 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 79) (slave01, executor 2, partition 0, PROCESS_LOCAL, 8810 bytes) \n24/12/23 11:54:00 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on slave02:34411 (size: 5.6 KiB, free: 67.7 MiB)\n24/12/23 11:54:00 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on slave01:38235 (size: 5.6 KiB, free: 65.5 MiB)\n24/12/23 11:54:01 INFO TaskSetManager: Starting task 3.0 in stage 18.0 (TID 80) (slave02, executor 1, partition 3, PROCESS_LOCAL, 8810 bytes) \n24/12/23 11:54:01 INFO TaskSetManager: Finished task 1.0 in stage 18.0 (TID 78) in 516 ms on slave02 (executor 1) (1/6)\n24/12/23 11:54:01 INFO TaskSetManager: Starting task 2.0 in stage 18.0 (TID 81) (slave01, executor 2, partition 2, PROCESS_LOCAL, 8810 bytes) \n24/12/23 11:54:01 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 79) in 561 ms on slave01 (executor 2) (2/6)\n24/12/23 11:54:01 INFO TaskSetManager: Starting task 5.0 in stage 18.0 (TID 82) (slave02, executor 1, partition 5, PROCESS_LOCAL, 8810 bytes) \n24/12/23 11:54:01 INFO TaskSetManager: Finished task 3.0 in stage 18.0 (TID 80) in 441 ms on slave02 (executor 1) (3/6)\n24/12/23 11:54:01 INFO TaskSetManager: Starting task 4.0 in stage 18.0 (TID 83) (slave01, executor 2, partition 4, PROCESS_LOCAL, 8810 bytes) \n24/12/23 11:54:01 INFO TaskSetManager: Finished task 2.0 in stage 18.0 (TID 81) in 430 ms on slave01 (executor 2) (4/6)\n24/12/23 11:54:01 INFO TaskSetManager: Finished task 5.0 in stage 18.0 (TID 82) in 310 ms on slave02 (executor 1) (5/6)\n24/12/23 11:54:02 INFO TaskSetManager: Finished task 4.0 in stage 18.0 (TID 83) in 372 ms on slave01 (executor 2) (6/6)\n24/12/23 11:54:02 INFO YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \n24/12/23 11:54:02 INFO DAGScheduler: ResultStage 18 (collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12) finished in 1.389 s\n24/12/23 11:54:02 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n24/12/23 11:54:02 INFO YarnScheduler: Killing all running tasks in stage 18: Stage finished\n24/12/23 11:54:02 INFO DAGScheduler: Job 8 finished: collect at /usr/local/spark3.4.4/tpch_queries/tpch_query_q3_rdd_MEMORY_ONLY.py:12, took 1.403444 s\n24/12/23 11:54:02 INFO PythonRDD: Removing RDD 6 from persistence list\n24/12/23 11:54:02 INFO BlockManager: Removing RDD 6\n24/12/23 11:54:02 INFO PythonRDD: Removing RDD 7 from persistence list\n24/12/23 11:54:02 INFO PythonRDD: Removing RDD 8 from persistence list\n24/12/23 11:54:02 INFO BlockManager: Removing RDD 7\n24/12/23 11:54:02 INFO BlockManager: Removing RDD 8\n24/12/23 11:54:02 INFO SparkContext: SparkContext is stopping with exitCode 0.\n24/12/23 11:54:02 INFO SparkUI: Stopped Spark web UI at http://master:4040\n24/12/23 11:54:02 INFO YarnClientSchedulerBackend: Interrupting monitor thread\n24/12/23 11:54:02 INFO YarnClientSchedulerBackend: Shutting down all executors\n24/12/23 11:54:02 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\n24/12/23 11:54:02 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped\n24/12/23 11:54:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n24/12/23 11:54:02 INFO MemoryStore: MemoryStore cleared\n24/12/23 11:54:02 INFO BlockManager: BlockManager stopped\n24/12/23 11:54:02 INFO BlockManagerMaster: BlockManagerMaster stopped\n24/12/23 11:54:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n24/12/23 11:54:02 INFO SparkContext: Successfully stopped SparkContext\n24/12/23 11:54:03 INFO ShutdownHookManager: Shutdown hook called\n24/12/23 11:54:03 INFO ShutdownHookManager: Deleting directory /usr/local/spark3.4.4/spark-8659bcb9-8f8a-4c96-990d-8fcc99ddd4f4/pyspark-04dae889-bb71-4e91-96d8-fa7cd13cfb17\n24/12/23 11:54:03 INFO ShutdownHookManager: Deleting directory /usr/local/spark3.4.4/spark-8659bcb9-8f8a-4c96-990d-8fcc99ddd4f4\n24/12/23 11:54:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-9ec3047b-83d7-47d4-a9d4-812ab7e7a8fa\n",
  "performance_metrics": "Performance Metrics:\n--------------------------------------------------\n\nData Size (MB):\nCustomer: 27.08\nOrders: 206.90\nLineitem: 1016.55\n\nData Skew Analysis:\nCustomer - Max/Min Ratio: 1.00\nOrders - Max/Min Ratio: 1.00\nLineitem - Max/Min Ratio: 1.52\n"
}